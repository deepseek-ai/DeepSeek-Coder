<p align="center">
<img width="1000px" alt="DeepSeek Coder" src="pictures/logo.jpeg">
</p>
<p align="center"><a href="">[ğŸ  ä¸»é¡µ]</a> | <a href="">[ğŸ¤– åœ¨çº¿ä½“éªŒ] | <a href="">[ğŸ¤— æ¨¡å‹ä¸‹è½½]</a> | <a href="">[ğŸ“„ è‹±æ–‡ç‰ˆ]</a> </p>
<hr>

### 1. Deepseek Coderç®€ä»‹

Deepseek Coder åŒ…æ‹¬ä¸€ç³»åˆ—é«˜çº§è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨87%çš„ä»£ç å’Œ13%çš„ä¸­è‹±æ–‡è‡ªç„¶è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå…±2Tçš„å•è¯ã€‚
Deepseek Coderæä¾›å„ç§å‚æ•°å¤§å°çš„ä»£ç æ¨¡å‹ï¼ŒèŒƒå›´ä»1Båˆ°33Bç‰ˆæœ¬ã€‚æ¯ä¸ªæ¨¡å‹éƒ½åœ¨é¡¹ç›®çº§ä»£ç æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé‡‡ç”¨16Kçš„çª—å£å¤§å°å’Œé¢å¤–çš„Fill-in-the-blankä»»åŠ¡ï¼Œä»¥æ”¯æŒé¡¹ç›®çº§åˆ«çš„ä»£ç è¡¥å…¨å’Œå¡«å……ã€‚
åœ¨ä»£ç èƒ½åŠ›æ–¹é¢ï¼ŒDeepseek Coderåœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œå„ç§æµ‹è¯•åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¾¾åˆ°äº†ç›®å‰å¼€æºä»£ç æ¨¡å‹çš„æœ€ä¼˜æ€§èƒ½ã€‚

- **å¤§é‡çš„è®­ç»ƒæ•°æ®**ï¼šåœ¨2Tå•è¯ä¸Šè®­ç»ƒï¼ŒåŒ…æ‹¬87%çš„ä»£ç å’Œ13%çš„è‹±æ–‡å’Œä¸­æ–‡è¯­è¨€æ•°æ®ã€‚

- **é«˜åº¦çµæ´»ä¸”å¯æ‰©å±•**ï¼šæä¾›1Bã€7Bå’Œ33Bçš„æ¨¡å‹å¤§å°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€‰æ‹©æœ€é€‚åˆå…¶éœ€æ±‚çš„æ¨¡å‹ã€‚

- **å“è¶Šçš„æ¨¡å‹æ€§èƒ½**ï¼šåœ¨ HumanEval-X, MultiPL-E, MBPP, DS-1000, å’Œ APPS åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDeepSeek Coderåœ¨å…¬å¼€å¯ç”¨çš„ä»£ç æ¨¡å‹ä¸­æ€§èƒ½æœ€ä¼˜ã€‚

- **å…ˆè¿›çš„ä»£ç è¡¥å…¨èƒ½åŠ›**ï¼šé‡‡ç”¨16Kçš„çª—å£å¤§å°å’ŒFill-in-the-blankè®­ç»ƒä»»åŠ¡ï¼Œæ”¯æŒé¡¹ç›®çº§ä»£ç è¡¥å…¨å’Œå¡«å……ä»»åŠ¡ã€‚

### 2. æ•°æ®å¤„ç†å’Œæ¨¡å‹è®­ç»ƒ

#### æ•°æ®å¤„ç†

- æ­¥éª¤1ï¼šä»GitHubæ”¶é›†ä»£ç æ•°æ®ï¼Œå¹¶é‡‡ç”¨ä¸[StarcoderData](https://github.com/bigcode-project/bigcode-dataset)ç›¸åŒçš„è¿‡æ»¤è§„åˆ™æ¥ç­›é€‰æ•°æ®ã€‚
- æ­¥éª¤2ï¼šè§£æåŒä¸€ä»“åº“ä¸­æ–‡ä»¶çš„ä¾èµ–å…³ç³»ï¼Œæ ¹æ®å®ƒä»¬çš„ä¾èµ–å…³ç³»é‡æ–°æ’åˆ—æ–‡ä»¶ä½ç½®ã€‚
- æ­¥éª¤3ï¼šç»„ç»‡ä¾èµ–æ–‡ä»¶ä»¥å½¢æˆå•ä¸€ç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨ä»“åº“çº§åˆ«çš„minhashè¿›è¡Œå»é‡ã€‚
- æ­¥éª¤4ï¼šè¿›ä¸€æ­¥è¿‡æ»¤æ‰ä½è´¨é‡çš„ä»£ç ï¼Œä¾‹å¦‚è¯­æ³•é”™è¯¯æˆ–å¯è¯»æ€§å·®çš„ä»£ç ã€‚

![Data Clean Procedure](pictures/data_clean.png)

#### æ¨¡å‹è®­ç»ƒ

- æ­¥éª¤1ï¼šé¦–å…ˆä½¿ç”¨å¤„ç†åæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®ç”±87%çš„ä»£ç ã€10%ä¸ä»£ç ç›¸å…³çš„è¯­è¨€æ•°æ®ï¼ˆGithub Markdownå’ŒStack Exchangeï¼‰ä»¥åŠ3%ä¸ä»£ç æ— å…³çš„ä¸­æ–‡è¯­è¨€æ•°æ®ç»„æˆã€‚åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œé‡‡ç”¨1.8Tçš„å•è¯å’Œ4Kçš„çª—å£å¤§å°è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒã€‚

- æ­¥éª¤2ï¼šæ‰©å±•çš„çª—å£è‡³16Kå¹¶ä½¿ç”¨é¢å¤–çš„200Bå•è¯è¿›ä¸€æ­¥çš„è¿›è¡Œé¢„è®­ç»ƒï¼Œä»è€Œå¾—åˆ°åŸºç¡€ç‰ˆæœ¬æ¨¡å‹ï¼ˆDeepSeek-Coder-Baseï¼‰ã€‚

- æ­¥éª¤3ï¼šä½¿ç”¨300Må•è¯çš„æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹ï¼ˆDeepSeek-Coder-Instructï¼‰ã€‚

![Model Pre-training](pictures/model_pretraining.png)


### 3. ä¸‹è½½å’Œç¯å¢ƒä¾èµ–

Deepseek Coder æœ€åˆæ˜¯åœ¨ Pytorch ä¸­å®ç°å¹¶åœ¨A100è¿›è¡Œè®­ç»ƒçš„ã€‚æˆ‘ä»¬æä¾›äº†åŸºäºHai-LLMçš„ pytorch å…¼å®¹ç‰ˆæœ¬ï¼Œæ”¯æŒtransformers(3.34+)ï¼Œä»¥ä¾¿åœ¨å…¶ä»–GPUå¹³å°ä¸Šä½¿ç”¨ã€‚

åŒæ—¶æ¨¡å‹çš„æƒé‡å·²ä¸Šä¼ åˆ°è‡³ğŸ¤— [huggingface](https://huggingface.co/deepseek-ai/deepseek-coder-7b)ã€‚

#### ç¯å¢ƒä¾èµ–
Python 3.8+ / CUDA 11+ / PyTorch 2.0+ / transformers 3.34+.

### 4. æ¨¡å‹æ¨ç†
è¯·å‚è€ƒä¸‹é¢æ ·ä¾‹æ¥ä½¿ç”¨æˆ‘ä»¬æ¨¡å‹ï¼š

#### ä»£ç è¡¥å…¨
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b")
device = 0 if torch.cuda.is_available() else -1
model = AutoModelForCausalLM.from_pretrained("deepseek/deepseek-coder-7b").to(device)
inputs = tokenizer("def hello_world():", return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

#### ä»£ç å¡«å……
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b")
device = 0 if torch.cuda.is_available() else -1
model = AutoModelForCausalLM.from_pretrained("deepseek/deepseek-coder-7b").to(device)
input_text = "<fim_prefix>def print_hello_world():\n    <fim_suffix>\n    print('Hello world!')<fim_middle>"
inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0]))
```

#### ä»“åº“çº§åˆ«çš„ä»£ç è¡¥å…¨
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b")
input_text = """#utils.py
import torch
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def load_data():
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target

    # Standardize the data
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Convert numpy data to PyTorch tensors
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.int64)
    y_test = torch.tensor(y_test, dtype=torch.int64)
    
    return X_train, X_test, y_train, y_test

def evaluate_predictions(y_test, y_pred):
    return accuracy_score(y_test, y_pred)
#model.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class IrisClassifier(nn.Module):
    def __init__(self):
        super(IrisClassifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(4, 16),
            nn.ReLU(),
            nn.Linear(16, 3)
        )

    def forward(self, x):
        return self.fc(x)

    def train_model(self, X_train, y_train, epochs, lr, batch_size):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)
        
        # Create DataLoader for batches
        dataset = TensorDataset(X_train, y_train)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = self(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

    def predict(self, X_test):
        with torch.no_grad():
            outputs = self(X_test)
            _, predicted = outputs.max(1)
        return predicted.numpy()
#main.py
from utils import load_data, evaluate_predictions
from model import IrisClassifier as Classifier

def main():
    # Model training and evaluation
"""
inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0]))
```

---
In the following scenario, the Deepseek-Coder 7B model effectively calls a class **IrisClassifier** and its member function from the `model.py` file, and also utilizes functions from the `utils.py` file, to correctly complete the **main** function in`main.py` file for model training and evaluation.

åœ¨ä¸‹é¢æ ·ä¾‹ä¸­ï¼ŒDeepseek-Coder 7B æ¨¡å‹æœ‰æ•ˆåœ°ä» `model.py` æ–‡ä»¶ä¸­è°ƒç”¨äº†ä¸€ä¸ªåä¸º `IrisClassifier` çš„ç±»åŠå…¶æˆå‘˜å‡½æ•°ï¼Œå¹¶åˆ©ç”¨äº† `utils.py` æ–‡ä»¶ä¸­çš„å‡½æ•°ï¼Œä»¥æ­£ç¡®åœ°å®Œæˆ`main.py` æ–‡ä»¶ä¸­çš„æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°çš„åŠŸèƒ½ã€‚

![Completion GIF](pictures/completion_demo.gif)

#### ChatåŠŸèƒ½
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b")
prompt = "write a quick sort algorithm in python."
prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\n\n### Instruction:\nWrite a program to perform the given task.\n\nInput:\n{prompt}\n\n### Response:\n"""
inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0]))
```

### 5. Lincense

### 6. Citation



