<p align="center">
<img width="1000px" alt="DeepSeek Coder" src="pictures/logo.jpeg">
</p>
<p align="center"><a href="https://www.deepseek.com/">[<img src="pictures/home.png" width="20px">ä¸»é¡µ]</a> | <a href="https://coder.deepseek.com/">[ğŸ¤– åœ¨çº¿ä½“éªŒ] | <a href="https://huggingface.co/deepseek-ai">[ğŸ¤— æ¨¡å‹ä¸‹è½½]</a> | <a href="README.md">[ğŸ“„ English Version] </a> </p>
<hr>


### 1. Deepseek Coderç®€ä»‹

Deepseek Coder åŒ…æ‹¬ä¸€ç³»åˆ—ä»£ç é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨87%çš„ä»£ç å’Œ13%çš„ä¸­è‹±æ–‡è‡ªç„¶è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå…±2Tçš„å•è¯ã€‚
Deepseek Coderæä¾›å„ç§å‚æ•°å¤§å°çš„ä»£ç æ¨¡å‹ï¼ŒèŒƒå›´ä»1Båˆ°33Bç‰ˆæœ¬ã€‚æ¯ä¸ªæ¨¡å‹éƒ½åœ¨é¡¹ç›®çº§ä»£ç æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé‡‡ç”¨16Kçš„çª—å£å¤§å°å’Œé¢å¤–çš„Fill-in-the-blankä»»åŠ¡ï¼Œä»¥æ”¯æŒé¡¹ç›®çº§åˆ«çš„ä»£ç è¡¥å…¨å’Œå¡«å……ã€‚åœ¨ä»£ç èƒ½åŠ›æ–¹é¢ï¼ŒDeepseek Coderåœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œå„ç§æµ‹è¯•åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¾¾åˆ°äº†ç›®å‰å¼€æºä»£ç æ¨¡å‹çš„æœ€ä¼˜æ€§èƒ½ã€‚

<img src="pictures/result.png" alt="result" width="85%">

- **æµ·é‡è®­ç»ƒæ•°æ®**ï¼šåœ¨2ä¸‡äº¿å•è¯ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬87%çš„ä»£ç å’Œ13%çš„è‹±æ–‡å’Œä¸­æ–‡è¯­è¨€æ•°æ®ã€‚

- **çµæ´»å¯æ‰©å±•**ï¼šæä¾›1Bã€7Bå’Œ33Bçš„æ¨¡å‹å¤§å°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€‰æ‹©æœ€é€‚åˆå…¶éœ€æ±‚çš„æ¨¡å‹ã€‚

- **æ¨¡å‹æ€§èƒ½å¼ºå¤§**ï¼šåœ¨ HumanEval, MultiPL-E, MBPP, DS-1000, å’Œ APPS åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDeepSeek Coderåœ¨å…¬å¼€å¯ç”¨çš„ä»£ç æ¨¡å‹ä¸­æ€§èƒ½æœ€ä¼˜ã€‚

- **é¡¹ç›®çº§ä»£ç è¡¥å…¨**ï¼šé‡‡ç”¨16Kçš„çª—å£å¤§å°å’ŒFill-in-the-blankè®­ç»ƒä»»åŠ¡ï¼Œæ”¯æŒé¡¹ç›®çº§ä»£ç è¡¥å…¨å’Œå¡«å……ä»»åŠ¡ã€‚

### 2. æ•°æ®å¤„ç†å’Œæ¨¡å‹è®­ç»ƒ

#### æ•°æ®å¤„ç†

- æ­¥éª¤1ï¼šä»GitHubæ”¶é›†ä»£ç æ•°æ®ï¼Œå¹¶é‡‡ç”¨ä¸[StarcoderData](https://github.com/bigcode-project/bigcode-dataset)ç›¸åŒçš„è¿‡æ»¤è§„åˆ™æ¥ç­›é€‰æ•°æ®ã€‚
- æ­¥éª¤2ï¼šè§£æåŒä¸€ä»“åº“ä¸­æ–‡ä»¶çš„ä¾èµ–å…³ç³»ï¼Œæ ¹æ®å®ƒä»¬çš„ä¾èµ–å…³ç³»é‡æ–°æ’åˆ—æ–‡ä»¶ä½ç½®ã€‚
- æ­¥éª¤3ï¼šç»„ç»‡ä¾èµ–æ–‡ä»¶ä»¥å½¢æˆå•ä¸€ç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨ä»“åº“çº§åˆ«çš„minhashç®—æ³•è¿›è¡Œå»é‡ã€‚
- æ­¥éª¤4ï¼šè¿›ä¸€æ­¥è¿‡æ»¤æ‰ä½è´¨é‡çš„ä»£ç ï¼Œä¾‹å¦‚è¯­æ³•é”™è¯¯æˆ–å¯è¯»æ€§å·®çš„ä»£ç ã€‚

<img src="pictures/data_clean.png" alt="data_creation" width="100%">

#### æ¨¡å‹è®­ç»ƒ

- æ­¥éª¤1ï¼šé¦–å…ˆä½¿ç”¨å¤„ç†åæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ•°æ®ç”±87%çš„ä»£ç ã€10%ä¸ä»£ç ç›¸å…³çš„è¯­è¨€æ•°æ®ï¼ˆGithub Markdownå’ŒStack Exchangeï¼‰ä»¥åŠ3%ä¸ä»£ç æ— å…³çš„ä¸­æ–‡è¯­è¨€æ•°æ®ç»„æˆã€‚åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œä½¿ç”¨4Kçš„çª—å£å¤§å°åœ¨1.8ä¸‡äº¿å•è¯ä¸Šè¿›è¡Œæ¨¡å‹çš„é¢„è®­ç»ƒã€‚

- æ­¥éª¤2ï¼šæ‰©å±•çš„çª—å£è‡³16Kå¹¶ä½¿ç”¨é¢å¤–çš„2åƒäº¿å•è¯è¿›ä¸€æ­¥è¿›è¡Œé¢„è®­ç»ƒï¼Œä»è€Œå¾—åˆ°åŸºç¡€ç‰ˆæœ¬æ¨¡å‹ï¼ˆ**DeepSeek-Coder-Base**ï¼‰ã€‚

- æ­¥éª¤3ï¼šä½¿ç”¨20äº¿å•è¯çš„æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹ï¼ˆ**DeepSeek-Coder-Instruct**ï¼‰ã€‚

<img src="pictures/model_pretraining.png" alt="model_pretraining" width="100%">


### 3. ä¸‹è½½å’Œç¯å¢ƒä¾èµ–

æˆ‘ä»¬æä¾›äº†åŸºäºHai-LLMçš„ pytorch å…¼å®¹ç‰ˆæœ¬ï¼Œæ”¯æŒtransformers(3.34+)ï¼Œä»¥ä¾¿åœ¨å…¶ä»–GPUå¹³å°ä¸Šä½¿ç”¨ã€‚

åŒæ—¶æ¨¡å‹çš„æƒé‡å·²ä¸Šä¼ åˆ°è‡³ [huggingface](https://huggingface.co/deepseek-ai)ã€‚

#### ç¯å¢ƒä¾èµ–
Python 3.8+ / CUDA 11+ / PyTorch 2.0+ / transformers 3.34+.

### 4. æ¨¡å‹æ¨ç†
è¯·å‚è€ƒä¸‹é¢æ ·ä¾‹æ¥ä½¿ç”¨æˆ‘ä»¬æ¨¡å‹ï¼š

#### 1ï¼‰ä»£ç è¡¥å…¨
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True).cuda()
input_text = "#write a quick sort algorithm"
inputs = tokenizer(input_text, return_tensors="pt").cuda()
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

è¿™æ®µä»£ç å°†è¾“å…¥ä»¥ä¸‹ç»“æœ:

```
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[0]
    left = []
    right = []
    for i in range(1, len(arr)):
        if arr[i] < pivot:
            left.append(arr[i])
        else:
            right.append(arr[i])
    return quick_sort(left) + [pivot] + quick_sort(right)
```

#### 2ï¼‰ä»£ç å¡«å……

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True).cuda()
input_text = """<fim_prefix>def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[0]
    left = []
    right = []
<fim_middle>
        if arr[i] < pivot:
            left.append(arr[i])
        else:
            right.append(arr[i])
    return quick_sort(left) + [pivot] + quick_sort(right)<fim_suffix>"""
inputs = tokenizer(input_text, return_tensors="pt").cuda()
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])
```

è¿™æ®µä»£ç å°†è¾“å…¥ä»¥ä¸‹ç»“æœ:

```
   for i in range(1, len(arr)):
```

#### 3ï¼‰ä»“åº“çº§åˆ«çš„ä»£ç è¡¥å…¨

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True).cuda()

input_text = """#utils.py
import torch
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def load_data():
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target

    # Standardize the data
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Convert numpy data to PyTorch tensors
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.int64)
    y_test = torch.tensor(y_test, dtype=torch.int64)
    
    return X_train, X_test, y_train, y_test

def evaluate_predictions(y_test, y_pred):
    return accuracy_score(y_test, y_pred)
#model.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class IrisClassifier(nn.Module):
    def __init__(self):
        super(IrisClassifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(4, 16),
            nn.ReLU(),
            nn.Linear(16, 3)
        )

    def forward(self, x):
        return self.fc(x)

    def train_model(self, X_train, y_train, epochs, lr, batch_size):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)
        
        # Create DataLoader for batches
        dataset = TensorDataset(X_train, y_train)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = self(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

    def predict(self, X_test):
        with torch.no_grad():
            outputs = self(X_test)
            _, predicted = outputs.max(1)
        return predicted.numpy()
#main.py
from utils import load_data, evaluate_predictions
from model import IrisClassifier as Classifier

def main():
    # Model training and evaluation
"""
inputs = tokenizer(input_text, return_tensors="pt").cuda()
outputs = model.generate(**inputs, max_new_tokens=140)
print(tokenizer.decode(outputs[0]))
```

---
åœ¨ä¸‹é¢æ ·ä¾‹ä¸­ï¼ŒDeepseek-Coder 7B æ¨¡å‹æœ‰æ•ˆåœ°ä» `model.py` æ–‡ä»¶ä¸­è°ƒç”¨äº†ä¸€ä¸ªåä¸º `IrisClassifier` çš„ç±»åŠå…¶æˆå‘˜å‡½æ•°ï¼Œå¹¶åˆ©ç”¨äº† `utils.py` æ–‡ä»¶ä¸­çš„å‡½æ•°ï¼Œä»¥æ­£ç¡®åœ°å®Œæˆ`main.py` æ–‡ä»¶ä¸­çš„æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°çš„åŠŸèƒ½ã€‚

![Completion GIF](pictures/completion_demo.gif)

#### 4ï¼‰å¯¹è¯åŠŸèƒ½
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek/deepseek-coder-7b-base", trust_remote_code=True).cuda()
prompt = "write a quick sort algorithm in python."
prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\n\n### Instruction:\nWrite a program to perform the given task.\n\nInput:\n{prompt}\n\n### Response:\n"""
inputs = tokenizer.encode(prompt, return_tensors="pt").cuda()
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0]))
```



### 5. è¯„æµ‹ç»“æœ

ä»¥ä¸‹è¯„æµ‹ç»“æœçš„å¤ç°ä»£ç å¯æŸ¥çœ‹[Evaluation](https://github.com/deepseek-ai/deepseek-coder/tree/main/Evaluation)ç›®å½•

#### 1) [HumanEval](https://github.com/deepseek-ai/deepseek-coder/tree/main/Evaluation/HumanEval)

Multilingual Base Models

| Model               | Size | Python | C++   | Java  | PHP   | TS    | C#    | Bash  | JS    | Avg   |
| ------------------- | ---- | ------ | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| code-cushman-001    | 12B  | 33.5%  | 31.9% | 30.6% | 28.9% | 31.3% | 22.1% | 11.7% | -     | -     |
| CodeShell           | 7B   | 35.4%  | 32.9% | 34.2% | 31.7% | 30.2% | 38.0% | 7.0%  | 33.5% | 30.4% |
| CodeGeeX2           | 6B   | 36.0%  | 29.2% | 25.9% | 23.6% | 20.8% | 29.7% | 6.3%  | 24.8% | 24.5% |
| StarCoderBase       | 16B  | 31.7%  | 31.1% | 28.5% | 25.4% | 34.0% | 34.8% | 8.9%  | 29.8% | 28.0% |
| CodeLLama     | 7B   | 31.7%  | 29.8% | 34.2% | 23.6% | 36.5% | 36.7% | 12.0% | 29.2% | 29.2% |
| CodeLLama     | 13B  | 36.0%  | 37.9% | 38.0% | 34.2% | 45.2% | 43.0% | 16.5% | 32.3% | 35.4% |
| CodeLLama     | 34B  | 48.2%  | 44.7% | 44.9% | 41.0% | 42.1% | 48.7% | 15.8% | 42.2% | 41.0% |
|                     |      |        |       |       |       |       |       |       |       |       |
| DeepSeek-Coder-Base  | 1B   | 34.8%  | 31.1% | 32.3% | 24.2% | 28.9% | 36.7% | 10.1% | 28.6% | 28.3% |
| DeepSeek-Coder-Base | 7B   | 49.4%  | 50.3% | 43.0% | 38.5% | 49.7% | 50.0% | 28.5% | 48.4% | 44.7% |
| DeepSeek-Coder-Base | 33B  | -      | -     | -     | -     | -     | -     | -     | -     | -     |

Instruction-Tuned Models
| Model               | Size | Python | C++   | Java  | PHP   | TS    | C#    | Bash  | JS    | Avg   |
| ------------------- | ---- | ------ | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| ChatGPT             | -    | 70.7%  | 50.3% | 54.5% | 52.2% | 62.3% | 64.6% | 34.8% | 60.9% | 52.2% |
| GPT-4               | -    | 82.3%  | 70.2% | 74.8% | 70.8% | 73.0% | 77.9% | 51.3% | 83.2% | 72.9% |
| WizardCoder         | 16B  | 51.8%  | 41.6% | 41.1% | 42.2% | 44.7% | 46.8% | 12.7% | 42.8% | 40.5% |
| Phind-CodeLlama     | 34B  | -      | -     | -     | -     | -     | -     | -     | -     | -     |
|                     |      |        |       |       |       |       |       |       |       |       |
| DeepSeek-Coder-Instruct | 1B   | -      | -     | -     | -     | -     | -     | -     | -     | -     |
| DeepSeek-Coder-Instruct | 7B   | -      | -     | -     | -     | -     | -     | -     | -     | -     |
| DeepSeek-Coder-Instruct | 33B  | -      | -     | -     | -     | -     | -     | -     | -     | -     |



#### 2) [Math Reasoning](https://github.com/deepseek-ai/deepseek-coder/tree/main/Evaluation/PAL-Math)

Multilingual Base Models

| Model          | Size | GSM8k | MATH  | GSM-Hard | SVAMP | TabMWP | ASDiv | MAWPS | Avg   |
| -------------- | ---- | ----- | ----- | -------- | ----- | ------ | ----- | ----- | ----- |
| CodeShell      | 7B   | 17.0% | 9.1%  | 18.2%    | 45.6% | 29.6%  | 46.6% | 56.8% | 31.8% |
| CodeGeex-2     | 7B   | 23.6% | 9.6%  | 22.4%    | 48.0% | 47.2%  | 46.9% | 66.0% | 37.7% |
| StarCoder-Base | 16B  | 27.3% | 11.5% | 24.2%    | 44.0% | 45.6%  | 54.9% | 73.4% | 40.1% |
| CodeLLama-Base | 7B   | 36.4% | 12.3% | 29.7%    | 57.6% | 58.4%  | 59.6% | 82.6% | 48.0% |
| CodeLLama-Base | 13B  | 44.2% | 15.5% | 42.4%    | 65.6% | 61.6%  | 65.3% | 85.3% | 54.3% |
| CodeLLama-Base | 34B  | 58.2% | 22.1% | 55.2%    | 77.2% | 69.6%  | 70.0% | 92.8% | 63.6% |
|                |      |       |       |          |       |        |       |       |       |
| DeepSeek-Coder-Base  | 1B   | 17.0% | 13.4% | 13.3%    | 39.2% | 42.4%  | 44.8% | 66.0% | 33.7% |
| DeepSeek-Coder-Base  | 7B   | 46.0% | 20.6% | 40.0%    | 67.2% | 71.2%  | 67.1% | 89.1% | 57.3% |
| DeepSeek-Coder-Base  | 33B  | -     | -     | -        | -     | -      | -     | -     | -     |


Instruction-Tuned Models
| Model         | Size | GSM8k | MATH  | GSM-Hard | SVAMP | TabMWP | ASDiv | MAWPS | Avg   |
| ------------- | ---- | ----- | ----- | -------- | ----- | ------ | ----- | ----- | ----- |
| ChatGPT       | -    | 78.6% | 38.7% | 67.6%    | 77.8% | 79.9%  | 81.0% | 89.4% | 73.3% |
| GPT-4         | -    | 94.2% | 51.8% | 77.6%    | 94.8% | 95.9%  | 92.6% | 97.7% | 86.4% |
|               |      |       |       |          |       |        |       |       |       |
| DeepSeek-Coder-Instruct | 1B   | -     | -     | -        | -     | -      | -     | -     | -     |
| DeepSeek-Coder-Instruct | 7B   | -     | -     | -        | -     | -      | -     | -     | -     |
| DeepSeek-Coder-Instruct | 33B  | -     | -     | -        | -     | -      | -     | -     | -     |

### 6. åè®®



### 7. è”ç³»æ–¹å¼

å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·æå‡ºraiseæˆ–é€šè¿‡ [agi_code@deepseek.com](mailto:agi_code@deepseek.com) ä¸æˆ‘ä»¬è”ç³»ã€‚



